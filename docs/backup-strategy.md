# üõ°Ô∏è Strat√©gie de Sauvegarde Globale

Les containers sont √©ph√©m√®res, mais les donn√©es ne le sont pas. La strat√©gie de sauvegarde est con√ßue pour garantir la r√©silience et la r√©cup√©ration rapide en cas de sinistre, tout en minimisant les risques de perte de donn√©es.

L'ensemble des services montent un volume Docker pointant vers `/srv/services/<service_name>/data` ou `srv/services/<service_name>/appdata/<container>` si n√©cessaire (plusieurs containers dans le service). C'est dans ces dossiers que sont stock√©es les donn√©es stateful (bases de donn√©es, configurations, etc.) qui sont √† sauvegarder pour ne pas perdre l'√©tat.

## üßä Snapshot local (cold cold backup)

Le script `/srv/scripts/backup.sh` s'ex√©cute chaque nuit √† 04h00 via un cronjob root.

- **M√©thode** : Arr√™t complet des services (`docker compose down`)
- **P√©rim√®tre** : `/srv/services`, `/etc/wireguard`, `/etc/ssh`.
- **Int√©grit√©** : Utilisation de `--absolute-names` pour une restauration sans erreur.

> **IMPORTANT**
> Id√©alement, je souhaitais faire un `docker stop` -> `archivage` -> `docker start` pour √©viter le reset des networks docker etc. Malheureusement, j'ai rencontr√© des probl√®mes d'int√©grit√© des donn√©es (base de donn√©es corrompue) avec cette m√©thode pour Loki et Prometheus. Le backup est donc l√©g√®rement plus long (quelques secondes) mais **fiable**.

## üíæ Archivage offsite

Le Mac pull les archives via SSH/Rsync √† 04h30 (ou √† la premi√®re connexion si le Mac est √©teint √† ce moment-l√†) avec `launchd`. Initialement j'avais fait un cronjob sur le mac mais si il est √©teint au moment du backup, c'est mort. üòÅ

- **R√©tention** : 3 occurrences sur la Pi, 10 sur le Mac, 10 dans le cloud.

## üõ†Ô∏è Proc√©dure de Restauration

D√©compression de l'archive via le playbook Ansible `restore.yaml`.

## üß™ Tests de restauration

```mermaid
graph TD
    subgraph "PROD : Raspberry Pi 5"
        T0[04:00 : docker compose down] --> T1[G√©n√©ration Tarball Cold Backup]
        T1 --> T2[docker compose up]
        T3[(Stockage local : 3 occurrences)]
    end

    subgraph "OFFSITE : Mac M4"
        T4[04:30 : rsync PULL via SSH / launchd] --> T5[(Stockage local : 10 occurrences)]
    end

    subgraph "CLOUD : Cold Storage"
        T5 --> T6[Upload chiffr√© : 10 occurrences]
    end

    T2 --> T3
    T3 -.-> T4
```

- **Test de restauration effectu√©** : 

```mermaid
sequenceDiagram
    participant A as Admin
    participant ANS as Ansible (Control Node)
    participant PI as Raspberry Pi (Host)
    participant MAC as Mac M4 (Backup Store)

    Note over A, PI: Simulation de sinistre (Data Loss)
    
    A->>ANS: Lancement site.yaml
    ANS->>PI: Provisioning (Packages, Arborescence, HDD Mount)
    PI-->>ANS: Host Ready
    
    A->>ANS: Lancement restore.yaml
    ANS->>MAC: R√©cup√©ration de la derni√®re archive
    MAC-->>ANS: Archive .tar.gz
    ANS->>PI: Injection & Extraction
    ANS->>PI: Correction UIDs (472, 10001, 65534)
    ANS->>PI: Red√©marrage des services
    ANS->>PI: Clear tarball
    PI-->>ANS: Data Restored
    
    PI-->>A: Services Healthy (Check https://*.home)
```

- Restauration compl√®te test√©e avec succ√®s.
- Proc√©dure :
    1. Arr√™t des services (`docker compose down`).
    2. On casse volontairement des trucs (ex: suppression d'un dossier de config, ou d'une base de donn√©es, modification sauvage de fichiers stateful).
    3. Ex√©cution du playbook Ansible `site.yaml` pour restaurer l'√©tat initial basique, c'est √† dire :
        - packages essentiels (Docker, WireGuard, vim, etc.)
        - Arborescence de `/srv` avec les bonnes permissions.
        - Check pr√©sence du HDD
    4. Ex√©cution du playbook Ansible `restore.yaml` pour restaurer les donn√©es √† partir de la derni√®re archive.
    5. V√©rification que tout est revenu √† la normale (services qui sont red√©marr√©s, donn√©es pr√©sentes, modifications sauvages non pr√©sentes dans l'archive plus pr√©sente)